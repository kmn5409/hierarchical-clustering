{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Lab4: Hierarchical Clustering + Density Based Clustering\n","\n","## Overview\n","In this lab you will learn about using Hierarchical Clustering methods like Agglomerative clustering and various linkage methods like Ward, Centroid etc. We will also later utilize the DBScan algorithm to identify clusters\n"],"metadata":{"id":"cejvpHDmQNX3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvN50_6-o3Ug"},"outputs":[],"source":["from sklearn.datasets import load_wine\n","import pandas as pd\n","import numpy as np\n","\n","wine = load_wine()\n","X = wine.data\n","y = wine.target\n","df = pd.DataFrame(X, columns=wine.feature_names)\n","\n","print(\"Shape:\", X.shape)\n","df.head()\n"]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n"],"metadata":{"id":"-cT5oNTvo6Vf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","\n","# Using Ward's method (minimizes variance within clusters)\n","Z = linkage(X_scaled, method=\"ward\")\n","\n","\n","plt.figure(figsize=(12, 5))\n","dendrogram(Z, truncate_mode=\"level\", p=5)  # truncate for readability\n","plt.title(\"Hierarchical Clustering Dendrogram (Wine)\")\n","plt.xlabel(\"Cluster index or sample\")\n","plt.ylabel(\"Distance\")\n","plt.show()\n"],"metadata":{"id":"ekgUnAu4o7dT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","\n","# -------------------------\n","# TODO Use Single method for the linkage method from scipy (part 1) (2 points)\n","# ------------------------\n","Z =\n","\n","# ----------------------------\n","# Implementation ends here\n","# ----------------------------\n","\n","plt.figure(figsize=(12, 5))\n","dendrogram(Z, truncate_mode=\"level\", p=5)  # truncate for readability\n","plt.title(\"Hierarchical Clustering Dendrogram (Wine)\")\n","plt.xlabel(\"Cluster index or sample\")\n","plt.ylabel(\"Distance\")\n","plt.show()\n"],"metadata":{"id":"Tk37y_hXqmn6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","\n","\n","# -------------------------\n","# TODO Use Complete method for the linkage method from scipy (part 2) (2 points)\n","# ------------------------\n","Z =\n","\n","# ----------------------------\n","# Implementation ends here\n","# ----------------------------\n","\n","plt.figure(figsize=(12, 5))\n","dendrogram(Z, truncate_mode=\"level\", p=5)  # truncate for readability\n","plt.title(\"Hierarchical Clustering Dendrogram (Wine)\")\n","plt.xlabel(\"Cluster index or sample\")\n","plt.ylabel(\"Distance\")\n","plt.show()\n"],"metadata":{"id":"4C_6rJ9CqvQF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","\n","# -------------------------\n","# TODO Use Average method for the linkage method from scipy (part 3) (2 points)\n","# ------------------------\n","Z =\n","\n","# ----------------------------\n","# Implementation ends here\n","# ----------------------------\n","\n","plt.figure(figsize=(12, 5))\n","dendrogram(Z, truncate_mode=\"level\", p=5)  # truncate for readability\n","plt.title(\"Hierarchical Clustering Dendrogram (Wine)\")\n","plt.xlabel(\"Cluster index or sample\")\n","plt.ylabel(\"Distance\")\n","plt.show()\n"],"metadata":{"id":"tnD8BCCWq5OU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","\n","# -------------------------\n","# TODO Use Centroid method for the linkage method from scipy (part 4) (2 points)\n","# ------------------------\n","Z =\n","\n","# ----------------------------\n","# Implementation ends here\n","# ----------------------------\n","\n","plt.figure(figsize=(12, 5))\n","dendrogram(Z, truncate_mode=\"level\", p=5)  # truncate for readability\n","plt.title(\"Hierarchical Clustering Dendrogram (Wine)\")\n","plt.xlabel(\"Cluster index or sample\")\n","plt.ylabel(\"Distance\")\n","plt.show()\n"],"metadata":{"id":"ZH_nZANhq_Bx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","from sklearn.metrics import adjusted_rand_score, silhouette_score\n","from sklearn.cluster import AgglomerativeClustering"],"metadata":{"id":"Tt897ujIHb3d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# Load and standardize data\n","# -----------------------------\n","wine = load_wine()\n","X = wine.data\n","y = wine.target\n","feature_names = wine.feature_names\n","target_names = wine.target_names\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)"],"metadata":{"id":"9UwTHg8mHgh6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------------------------------------------\n","# TODO Agglomerative Clustering with Ward linkage with euclidean metric  (part 5) (2 points)\n","# ---------------------------------------------\n","\n","agg =\n","cluster_labels = agg.fit_predict(X_scaled)\n","\n","# ----------------------------\n","# Implementation ends here\n","# ----------------------------\n","\n","# -------------------------------------------------\n","# PCA to 2D (for visualization of any high-D X)\n","# -------------------------------------------------\n","pca = PCA(n_components=2, random_state=0)\n","X_pca = pca.fit_transform(X_scaled)\n","\n","# Put in a DataFrame for convenience\n","df = pd.DataFrame({\n","    \"PC1\": X_pca[:, 0],\n","    \"PC2\": X_pca[:, 1],\n","    \"Cluster\": cluster_labels,\n","    \"Class\": y\n","})\n","\n","# ---------------------------------------------\n","# Evaluate clustering quality (optional)\n","# ---------------------------------------------\n","ari = adjusted_rand_score(y, cluster_labels)\n","sil = silhouette_score(X_scaled, cluster_labels, metric=\"euclidean\")\n","print(f\"Adjusted Rand Index (vs true classes): {ari:.3f}\")\n","print(f\"Silhouette Score (on standardized features): {sil:.3f}\")\n","\n","# ---------------------------------------------\n","# Visualize clusters in PCA space\n","# ---------------------------------------------\n","plt.figure(figsize=(8, 6))\n","scatter = plt.scatter(df[\"PC1\"], df[\"PC2\"], c=df[\"Cluster\"], alpha=0.8)\n","plt.xlabel(\"PCA Component 1\")\n","plt.ylabel(\"PCA Component 2\")\n","plt.title(\"Agglomerative Clustering on Wine (PCA projection) — Cluster Labels\")\n","# Build a legend mapping cluster IDs to colors\n","handles, _ = scatter.legend_elements()\n","plt.legend(handles, [f\"Cluster {i}\" for i in range(3)], title=\"Clusters\")\n","plt.tight_layout()\n","plt.show()\n","\n","# ---------------------------------------------\n","# Visualize TRUE classes in the same PCA space\n","# ---------------------------------------------\n","plt.figure(figsize=(8, 6))\n","scatter_true = plt.scatter(df[\"PC1\"], df[\"PC2\"], c=df[\"Class\"], alpha=0.8)\n","plt.xlabel(\"PCA Component 1\")\n","plt.ylabel(\"PCA Component 2\")\n","plt.title(\"Wine True Classes (PCA projection)\")\n","handles_true, _ = scatter_true.legend_elements()\n","plt.legend(handles_true, [name.title() for name in target_names], title=\"True Class\")\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"hlauLV2Bo870"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------------------------------------------\n","# TODO Agglomerative Clustering with SINGLE linkage with euclidean metric (part 6) (2 points)\n","# ---------------------------------------------\n","agg =\n","cluster_labels = agg.fit_predict(X_scaled)\n","\n","# ----------------------------\n","# Implementation ends here\n","# ----------------------------\n","\n","# -------------------------------------------------\n","# PCA to 2D (for visualization of high-D X)\n","# -------------------------------------------------\n","pca = PCA(n_components=2, random_state=0)\n","X_pca = pca.fit_transform(X_scaled)\n","\n","df = pd.DataFrame({\n","    \"PC1\": X_pca[:, 0],\n","    \"PC2\": X_pca[:, 1],\n","    \"Cluster\": cluster_labels,\n","    \"Class\": y\n","})\n","\n","# ---------------------------------------------\n","# Evaluate clustering quality (optional)\n","# ---------------------------------------------\n","ari = adjusted_rand_score(y, cluster_labels)\n","sil = silhouette_score(X_scaled, cluster_labels, metric=\"euclidean\")\n","print(f\"Adjusted Rand Index (vs true classes): {ari:.3f}\")\n","print(f\"Silhouette Score (on standardized features): {sil:.3f}\")\n","\n","# ---------------------------------------------\n","# Visualize clusters in PCA space\n","# ---------------------------------------------\n","plt.figure(figsize=(8, 6))\n","scatter = plt.scatter(df[\"PC1\"], df[\"PC2\"], c=df[\"Cluster\"], alpha=0.8, cmap=\"rainbow\")\n","plt.xlabel(\"PCA Component 1\")\n","plt.ylabel(\"PCA Component 2\")\n","plt.title(\"Agglomerative Clustering on Wine (Single Linkage, PCA projection)\")\n","handles, _ = scatter.legend_elements()\n","plt.legend(handles, [f\"Cluster {i}\" for i in range(3)], title=\"Clusters\")\n","plt.tight_layout()\n","plt.show()\n","\n","# ---------------------------------------------\n","# Visualize TRUE classes in the same PCA space\n","# ---------------------------------------------\n","plt.figure(figsize=(8, 6))\n","scatter_true = plt.scatter(df[\"PC1\"], df[\"PC2\"], c=df[\"Class\"], alpha=0.8, cmap=\"rainbow\")\n","plt.xlabel(\"PCA Component 1\")\n","plt.ylabel(\"PCA Component 2\")\n","plt.title(\"Wine True Classes (PCA projection)\")\n","handles_true, _ = scatter_true.legend_elements()\n","plt.legend(handles_true, [name.title() for name in target_names], title=\"True Class\")\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"iXVZYztQrSjG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.cluster import AgglomerativeClustering\n","\n","# Try with k=3 (since wine has 3 classes)\n","model = AgglomerativeClustering(n_clusters=3, linkage=\"single\")\n","labels = model.fit_predict(X_scaled)\n","\n","print(\"Cluster labels:\", labels[:20])\n"],"metadata":{"id":"FLwZCjWrOyAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","np.unique(labels)"],"metadata":{"id":"Hae7MTLtOzLP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n","\n","\n","# ---------------------------------------------\n","# TODO Make a list of the various linkage methods namely single, complete, average and ward (part 7) (2 points)\n","# ---------------------------------------------\n","linkages = []\n","\n","# ----------------------------\n","# Implementation ends here\n","# ----------------------------\n","\n","for link in linkages:\n","    # ---------------------------------------------\n","    # TODO Call AgglomerativeClustering and set the number of clusters to be 3 with the corresponding linkage and set the metric to be euclidean (part 8) (2 points)\n","    # ---------------------------------------------\n","    model =\n","\n","    # ----------------------------\n","    # Implementation ends here\n","    # ----------------------------\n","\n","    labels = model.fit_predict(X_scaled)\n","    ari = adjusted_rand_score(y, labels)\n","    nmi = normalized_mutual_info_score(y, labels)\n","    sil = silhouette_score(X_scaled, labels)\n","    print(f\"{link:8s} | Silhouette={sil:.3f} | ARI={ari:.3f} | NMI={nmi:.3f}\")\n"],"metadata":{"id":"ynuwlmT3o_Tg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","\n","\n","# ---------------------------------------------\n","# TODO Use the clustermap from Seaboarn and use the X_scaled data, and set the method to be \"ward\", cmap to be \"viridis\" and figsize=(10,6) (part 9) (2 points)\n","# ---------------------------------------------\n","sns.\n","\n","# ----------------------------\n","# Implementation ends here\n","# ----------------------------\n","\n","plt.show()\n"],"metadata":{"id":"qeGrEYPjpAa1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ---------------------------------------------\n","# TODO Use the clustermap from Seaboarn and use the X_scaled data, and set the method to be \"single\", cmap to be \"viridis\" and figsize=(10,6) (part 10) (2 points)\n","# ---------------------------------------------\n","sns.\n","\n","# ----------------------------\n","# Implementation ends here\n","# ----------------------------\n","\n","plt.show()"],"metadata":{"id":"5t1yacXuI1yn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.datasets import make_blobs\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import AgglomerativeClustering\n","\n","# Generate synthetic 2D data (3 blobs)\n","X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.8, random_state=42)\n","\n","# Scale for better distance-based clustering\n","X_scaled = StandardScaler().fit_transform(X)\n","\n","\n","# ---------------------------------------------\n","# TODO Run Agglomerative Clustering (Ward linkage, Euclidean distance) and then fit_predict on the X_scaled data (part 11) (2 points)\n","# ---------------------------------------------\n","agg =\n","labels = agg.\n","\n","# ----------------------------\n","# Implementation ends here\n","# ----------------------------\n","\n","# Visualize the cluster labels\n","plt.figure(figsize=(6,5))\n","plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap=\"viridis\", s=40)\n","plt.title(\"Agglomerative Clustering on 2D data\")\n","plt.xlabel(\"Feature 1\")\n","plt.ylabel(\"Feature 2\")\n","plt.show()\n"],"metadata":{"id":"ty-ys-E3tTqF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.datasets import make_blobs\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import AgglomerativeClustering\n","\n","# Generate synthetic 2D data (3 blobs)\n","X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.8, random_state=42)\n","\n","# Scale for better distance-based clustering\n","X_scaled = StandardScaler().fit_transform(X)\n","\n","\n","# ---------------------------------------------\n","# TODO Run Agglomerative Clustering (Single linkage, Euclidean distance) and then fit_predict on the X_scaled data (part 12) (2 points)\n","# ---------------------------------------------\n","agg =\n","labels = agg.\n","\n","# ----------------------------\n","# Implementation ends here\n","# ----------------------------\n","\n","\n","# Visualize the cluster labels\n","plt.figure(figsize=(6,5))\n","plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap=\"viridis\", s=40)\n","plt.title(\"Agglomerative Clustering on 2D data\")\n","plt.xlabel(\"Feature 1\")\n","plt.ylabel(\"Feature 2\")\n","plt.show()\n"],"metadata":{"id":"Avh_T6RNtW4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.datasets import make_moons\n","\n","# 2 moons with some noise\n","X, y_true = make_moons(n_samples=400, noise=0.08, random_state=42)\n","\n","plt.figure(figsize=(5,4))\n","plt.scatter(X[:,0], X[:,1], s=20)\n","plt.title(\"Two Moons dataset\")\n","plt.show()\n"],"metadata":{"id":"TZW22LqDt5n3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","X_scaled = StandardScaler().fit_transform(X)\n"],"metadata":{"id":"DGfMbGQixLGE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.neighbors import NearestNeighbors\n","import numpy as np\n","\n","#Look for the “elbow” or point of maximum curvature in the plot.\n","#That elbow indicates a good choice of ε:\n","#Below it: points within dense clusters (small distances).\n","#Above it: noise/outliers (large distances).\n","\n","def k_distance_plot(X, k=5):\n","    nn = NearestNeighbors(n_neighbors=k).fit(X)\n","    distances, _ = nn.kneighbors(X)\n","    kth = np.sort(distances[:, -1])\n","    plt.figure(figsize=(6,4))\n","    plt.plot(kth)\n","    plt.title(f\"k-distance plot (k={k})\")\n","    plt.xlabel(\"Points sorted by distance\")\n","    plt.ylabel(f\"{k}-NN distance\")\n","    plt.show()\n","\n","k_distance_plot(X_scaled, k=5)\n"],"metadata":{"id":"qwD_8ry0xMwq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.cluster import DBSCAN\n","\n","\n","# ---------------------------------------------\n","# TODO (part 13) (2 points)\n","# ---------------------------------------------\n","eps =       # pick from k-distance elbow\n","min_samples = 5 # common default\n","\n","#Use DBSCAN with the eps and min_samples you defined above\n","db =\n","\n","#Run fit_predict on the X_scaled data\n","labels = db.\n","\n","# ----------------------------\n","# Implementation ends here\n","# ----------------------------\n","\n","print(\"Unique labels:\", set(labels))  # -1 means noise\n"],"metadata":{"id":"gFKdK6MAxN8y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(5,4))\n","plt.scatter(X_scaled[:,0], X_scaled[:,1], c=labels, cmap=\"viridis\", s=25)\n","plt.title(f\"DBSCAN on Two Moons (eps={eps}, min_samples={min_samples})\")\n","plt.xlabel(\"Feature 1\")\n","plt.ylabel(\"Feature 2\")\n","plt.show()\n"],"metadata":{"id":"-xulAVSOxPNa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# [TODO] Answer the following questions (6 points)\n","\n","---\n","\n","**1. Question:** What do the parameters ε (epsilon) and min_samples mean in DBSCAN?\n","\n","**Answer:**  \n","\n","---\n","\n","**2. Question:** Why did we use the value of 3 for the number of cluster centers in AgglomerativeClustering for the Wine dataset\n","\n","**Answer:**\n","\n","---\n","\n","**3. Question:** Why was the clustering for the single linkage method much worse compared to the ward linkage\n","\n","**Answer:**\n"],"metadata":{"id":"gYehlSmILAru"}}]}